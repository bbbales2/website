<!DOCTYPE html>
<html>
  <head>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.2.0/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/underscore.js/1.8.3/underscore.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/backbone.js/1.2.3/backbone.js"></script>

    <link rel="stylesheet" href="http://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css">
    <script src="http://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.js"></script>

    <script src="static/js/index.js"></script>

    <link rel="stylesheet" type="text/css" href="static/css/index.css">
  </head>
  <body>
    <div class="document">
      <div class="doctitle">Segmentation of Superalloy Microstructures</div>
      <div class="section">
        <div class="title">Introduction and Background</div>
        <p>
          Electron back scatter (EBS) micrographs are important for superalloy studies. Short of optical imaging, it's the easiest measurement you can make on a superalloy system. Superalloys systems themselves are defined by a an intricate pattern of two interlaced materials known as a y-y' microstructure. EBS images give us is a grayscale map of different materials on a surface, so, if we cut open superalloys samples and take EBS micrographs of the surface, we can study the y-y' microstructure of a superalloy. Look at <a class="ref" data-ref="rene88"></a> and <a class="ref" data-ref="renen4"></a> for two example EBS micrographs. Tunneling electron microscope (TEM) and electron back scatter diffraction (EBSD) technologies are also frequently used in microstructure analysis. The techniques described here can all be used for TEM data, but do not apply to EBSD samples.
        </p>

        <div class="imgDiv">
          <div class="img rene88">
            <img src="static/images/rene88plot.png">
            <div class="caption">EBS image of Rene 88, superalloy used in gas turbine cores</div>
          </div>
          
          <div class="img renen4">
            <img src="static/images/renen4plot.png">
            <div class="caption">EBS image of Rene N4, superalloy used in gas turbine blades</div>
          </div>
        </div>

        <p>
          What exactly these microstructure images tell us varies as a function of the study. Frequently, scientists take micrographs looking for microstructural abnormalities. They sit at a microscope and scroll around the surface looking for something interesting. They could be looking for carbides -- big lumps of material that can induce crack growth in a sample, or they could be looking for the cracks themselves. This is not what we're interested in in this particular segmentation tutorial, but it's important in other things we do.
        </p>

        <p>
          Another reason scientists look at micrographs is to figure out how a superalloy reacted to either 1) an extreme load that represents some sort of upper operating limit for a part or 2) a growth or rejuvination process used to produce new superalloy parts or repair old ones. Superalloys are used in gas turbines, and these are unusually harsh operating conditions. During the life of a turbine blade, for instance, it can actually stretch significantly (easily a few percent of its original length). This has a dramatic effect on the microstructure. <a class="ref" data-ref="renen5strain0"></a> and <a class="ref" data-ref="renen5strain2"></a> show a before and after of a ReneN5 superalloy (this corresponds to the first kind of test). It's also reasonable that a scientists might grow various superalloy samples with different compositions to see how the microstructures change. Samples of this are given in <a class="ref" data-ref="moly0"></a> and <a class="ref" data-ref="moly1"></a> (this corresponds to the second kind of test).
        </p>

        <div class="imgDiv">
          <div class="img renen5strain0">
            <img src="static/images/renen5strain0.png">
            <div class="caption">EBS image of a ready-to-go ReneN5 sample.</div>
          </div>

          <div class="img renen5strain2">
            <img src="static/images/renen5strain2.png">
            <div class="caption">EBS image of a ReneN5 sample after it's been stretched to 102% of its original length.</div>
          </div>

          <div class="img moly0">
              <img src="static/images/moly0.png">
              <div class="caption">A TEM of a superalloy grown with 2% Molybdenum atomic %.</div>
          </div>

          <div class="img moly1">
              <img src="static/images/moly1.png">
              <div class="caption">A TEM of a superalloy grown with 8% Molybdenum atomic %.</div>
          </div>
        </div>

        <p>
          These are all reasons why we might want to analyze microstructure images. Now let's talk about how. Image segmentation is one tool used for this.
        </p>
      </div>

      <div class="section">
        <div class="title">Image segmentation</div>
        <p>
          Image segmentation is pretty simple -- it's the process by which pixels in an image are classified into a number of discrete categories. Look at the Wikipedia page: <a class="extern" href="https://en.wikipedia.org/wiki/Image_segmentation#Applications"></a>, or this nice MathWorks page <a class="extern" href="http://www.mathworks.com/discovery/image-segmentation.html"></a> for some background and perspective. In our case we are interested in classifying pixels in superalloy micrographs (images) into either gamma or gamma-prime.
        </p>
        <div class="sub-section">
          <div class="title">Data, and Problems with that Data</div>
          <p>
            So I've already introduced EBS and TEM image samples. These are the datasets I mostly work with. They're mostly produced at the University of California, Santa Barbara by researchers in Tresa Pollock's group. Sometimes I work with collections of 2D images, and sometimes I work with stacks of images that represent a 3D volume (<a href="http://dx.doi.org/10.1063/1.3680111">Tri-Beam</a>).
          </p>

          <p>
            EBS images can have a fair bit of uncorrelated spectle noise (see again <a class="ref" data-ref="rene88"></a> and <a class="ref" data-ref="renen4"></a>). In some applications this is avoidable, but in others it's not.
          </p>

          <p>
            EBS and TEM images, though they produce 2D micrographs, are actually sampling shallow 3D volumes. In TEM images, electrons are actually being fired through a thin slice of material and detected on the opposite side. In EBS images, electrons pierce into a surface before bouncing back out and hitting a detector (hence the "back scatter" in electron back scatter). This can cause issues in images where it's unclear if a feature you're seeing is actually at the surface or just slightly below.
          </p>

          <p>
            Crystal orientation information is lost in EBS and TEM images. More importantly for the samples here, information about anti-phase boundaries are lost as well. I haven't talked about it yet, but these days our turbine blades are mostly single crystal. This means all the atoms in the material are aligned on a single grid. As well as being aligned to a grid, all the different atoms in the alloy are arranged in different patterns throughout the material. The different y and y' materials are simply areas where the proportion of atoms is different, but they're still on one grid. <b>An anti-phase boundary forms when the patterns of atoms in different areas are shifted so that different regions of the same composition and crystal don't quite fit together.</b>
          </p>

          <p>
            As an example, I've drawn a possible 1D nucleation and growth process that produces an antiphase boundary below. Our samples grow via spinodal decomposition (not nucleation and growth) but the illustration still applies:
          </p>

          <pre>
             time |
                1 | ....................
                2 | ..A.................
                3 | ..AB..........B.....
                4 | ..ABAB........B.....
                5 | ..ABABA......AB.....
              ... | ....................
            T + 1 | ..ABABABA.BABAB.....
            T + 2 | ..ABABABA<font color="red"><u><b>A</b></u></font>BABAB.....
          </pre>

          <p>
            At time 2 the first nuclei appears. At time 3, the first nuclei has growth a little, and the second nuclei appears. The two nuclei grow over time, and eventually are only a single atom apart (t == T + 1). Whatever atom fills that spot, A or B, the natural crystal pattern will be messed up. This is demonstrated at time T + 2.
          </p>

          <p>
            These sorts of boundaries are interesting to us because they can be very difficult to detect. Frequently their existence must be inferred from the global microstructure. <a class="ref" data-ref="antiphase"></a> is a good demonstration of this.
          </p>

          <div class="imgDiv">
            <div class="img antiphase">
              <img src="static/images/antiphase.png">
              <div class="caption">TEM image from above. From observing the image, we expect precipitates to be square. In the three circled instances we see two precipitates spaced so close that there is no visible spacing between them. It's likely antiphase boundaries are what's allowing these precipitates to get so close. It will be difficult to pull the precipitates apart with segmentation.</div>
            </div>
          </div>
          
          <p>
            Alright, now that we've looked at our data and understand the problems associated with it, it's time to look at what a very basic segmentation algorithm produces. are algorithms segmented by first applying a Gaussian blur to get rid of noise and then a threshold segmentation (any image above a given value is one class and everything below is another). The threshold level was determined by an Otsu threshold algorithm (for simple thresholding, this is probably gonna be a good choice most of the time). Example blurred microstructures are in <a class="ref" data-ref="rene88blur"></a> and <a class="ref" data-ref="renen4blur"></a> and then the thresholded ones are in <a class="ref" data-ref="rene88seg"></a> and <a class="ref" data-ref="renen4seg"></a>. The code for this can be found at <a href="https://github.com/bbbales2/website/blob/master/seg/python/otsu_example.py">Github</a>.
        </p>

        <div class="imgDiv">
          <div class="img rene88blur">
            <img src="static/images/rene88blur.png">
            <div class="caption">The is the Rene 88 sample from <a class="ref" data-ref="rene88"></a> with a slight blur applied. I'm showing this just to convince you that no "important" structure is destroyed by this blurring process. Hopefully you are convinced.</div>
          </div>
          
          <div class="img renen4blur">
            <img src="static/images/renen4blur.png">
            <div class="caption">The is the Rene N4 sample from <a class="ref" data-ref="renen4"></a> with a slight blur applied. Again, hopefully this shows the slight blur isn't destroying structure. What we're doing is saying "we want the large objects from this image; ignore the small ones." This idea of using blurring operations to achieve this goal is quite old</div>
          </div>

          <div class="img rene88seg">
            <img src="static/images/rene88otsu.png">
            <div class="caption">The is the blurred Rene 88 sample from <a class="ref" data-ref="rene88blur"></a> with an Otsu Threshold segmentation applied</div>
          </div>
          
          <div class="img renen4seg">
            <img src="static/images/renen4otsu.png">
            <div class="caption">The is the blurred Rene N4 sample from <a class="ref" data-ref="renen4blur"></a> with an Otsu Threshold segmentation applied</div>
          </div>
        </div>

        <p>
          Now, before we get into much more detail about segmentation algorithms, let me say I think this sort of segmentation does a pretty good job. It's probably classifying 90% of the pixels according to what a human would. What these simple segmentation algorithms do get wrong are the edges, and why we deem them inadequate is these errors around precipitate edges compromise the assumptions of the postprocessing we want to apply to the segmented image. For instance, we want to do things like measure precipitate size or aspect ratios. If precipitates get merged together, these statistics are compromised. This would not be the case if we were doing something like comparing percent y to percent y'.
        </p>

        <div class="imgDiv">
          <div class="img renen4otsuBordersMarked">
            <img src="static/images/renen4otsuBordersMarked.png">
            <div class="caption">These issues are pretty prevalent in any segmentation technique. The green line approximately segments the image in half</div>
          </div>
        </div>

        <p>
          Let's quickly enumerate the outstanding issues with the above segmentations.
        </p>

        <ol>
          <li>Because of classification mistakes, islands get blurred together when they should probably be separate (look at the red circles in <a class="ref" data-ref="renen4otsuBordersMarked"></a>)</li>
          <li>The variation in background intensity from one side of the image to the other causes pixel misclassifications. I won't do any rigorous tests to prove this, but suffice to say the mean pixel value of the left side of the image is about 175, the mean value on the right is 181, and the threshold is about 178. There is some average different in pixel intensity that becomes important when performing the segmentation</li>
          <li>There's a fair number of possibly misclassified pixels either in the matrix or the precipitates (look where the blue arrows point in <a class="ref" data-ref="renen4otsuBordersMarked"></a>). Most of the time if a precipitate is unusually smaller than anything else in the image we assume it is noise. Naturally this can lead to inconsistencies</li>
        </ol>

        <p>
          That said, there are numerous segmentation algorithms that try to do better than the one outlined above. Some succeed and many fail. For this problem, I've mostly looked at untrained classifiers. At the most basic level, this is something like the Otsu thresholding or perhaps a <a href="https://en.wikipedia.org/wiki/K-means_clustering">K-means clustering</a>. Slightly more sophisticated than this is a Cahn-Hilliard segmentation (with a convenient, nice, physical interpretation) that I will describe here. A nice statistics flavored model known as <a href="http://dx.doi.org/10.1088/0965-0393/17/2/025002">EMMPM</a> (<a href="http://dx.doi.org/10.1109/83.869185">also</a> useful) is also used in superalloys segmentations. Finally, the Chan-Vese segmentation algorithm from image analysis is quite useful, though I have trouble withe Math.
        </p>

        <p>What we've found is that, of the problems listed above, the noise can be addressed most effectively. This is done by thinking of intensities in our EBS images as concentrations in a 2D spinodal decomposition process. We know that the concentration variations represented by the noise would be unstable in an actual spinodal decomposition, so if we're able to simulate this process the noise will be removed. Becuase the microstructures in our gamma-gamma prime superalloys form through a binary spinodal decomposition, the gamma-gamma prime microstructure in the micrographs should be stable.</p>
        <p>The second problem (the variation in background intensity) can be addressed through using commonly available local equalization techniques <a href="http://scikit-image.org/docs/dev/api/skimage.exposure.html#equalize-adapthist"></a>. The downside to these techniques being larger amounts of noise. However, while the spinodal decomposition cannot readily handle concentration variations across the whole micrograph, it can readily handle the extra local noise introduced by the local equalization.</p>
        <p>The final problem, that of accidentally merging islands, was never satisfactorily resolved. The fundamental reason it is so difficult to separate precipitates that butt up against each other stems from the fact that it is antiphase boundary issues, or single atom displacements, that allow precipitates to be very close to each other without merging together. These antiphase boundaries are not easy to directly observe with the data collection apparatus we are using.</p>
      </div>
      </div>
      <div class="section">
        <div class="title">
          Cahn-Hilliard Spinodal Decomposition for Segmentation
        </div>
        <p>Wikipedia defines a spinodal decomposition as:</p>

        <div class="quote">"Spinodal decomposition is a mechanism for the rapid unmixing of a mixture of liquids or solids from one thermodynamic phase, to form two coexisting phases."</div>

        <p>For a binary system (one that only has only two mixing species), spinodal decompositions boils down to three basic rules:</p>
        <ol>
          <li>No material is created or destroyed in the process. Total concentrations are conserved</li>
          <li>Material moves by diffusion. This means large gradients in concentration in the system are unstable (or equivalently interfaces represent a high energy cost)</li>
          <li>Concentrations in the system try to take values that minimize something called the free-energy functional</li>
        </ol>

        <p>For NiCu, the free-energy functional discussed above looks something like <img src="seg/static/images/NiCu.png">. In that plot, system concentration is the x-axis. It is represented by a value between zero and one. Zero being purely Ni, and one being purely Cu. The free-energy functional attains a minimum at point Y, and so if concentrations weren't conserved, then system would just wiggle around until the concentration was Y at every point in the system. However, because concentrations *are* conserved, minimizing concentrations is not trivial, and you have a tradeoff between all three properties listed above. The action of a randomly initialized system for this NiCu free-energy functional is diagrammed in <img src="seg/static/images/NiCuDecomp.png">. Look at that figure and think about how each of the properties outlined above effect the system decomposition. They all work together, so they aren't really processes you can deconvolute, but each plays an important role.</p>

        <p>The only trick we need to get this to work like a segmentation is to use a free-energy functional like the one in <img src="seg/static/images/binaryalloy.png">. This free-energy functional is totally artificial, but, with this functional, the concentrations of the system will segment into either the values -1 or 1 and make segmentation very easy. To convince you of the utility, look at the decomposition in <img src="seg/static/images/binaryalloyDecomp.png">. We start with a step edge, add a large amount of Gaussian noise, and then use a Cahn-Hilliard segmentation to recover the step edge from the noise. This is basically it.</p>
        <p>The simplest way to model these rules mathematically is the Cahn-Hilliard equation (1) for spinodal decomposition. These dynamics comes from minimizing the equation (2) driven by diffusion. More information can be found by searching <a href="http://www.rpgroup.caltech.edu/~natsirt/spinodal.pdf">1</a>, <a href="http://elie.korea.ac.kr/~cfdkim/papers/CHreview.pdf">2</a>, <a href="https://en.wikipedia.org/wiki/Cahn%E2%80%93Hilliard_equation">3</a>. The Cahn-Hilliard equation can easily be extended to 2D or 3D.</p>

        <ul class="eqs">
          <li><div class="eq">\frac{\partial c}{\partial t} = D \Delta \left ( c^3 - c - \gamma \Delta c \right )</div></li>
          <li><div class="eq">F(c) = \int_{V} \left ( \frac{\gamma}{2}\left | \nabla c \right |^2 + f \right ) d v</div></li>
        </ul>

        <div class="section">
          <div class="title">Examples</div>
          <div class="section">
            <div class="title">2D</div>
            <img src="">

            Example code for a 2D segmentation can be found at Github: <a href="https://github.com/bbbales2/ch/blob/master/ch2d.py">2d Cahn-Hilliard</a>
          </div>
          <div class="section">
            <div class="title">3D</div>
          </div>

          Example code for a 3D segmentation can be found at Github: <a href="https://github.com/bbbales2/ch/blob/master/chseg.py">3d Cahn-Hilliard</a>
        </div>
      </div>
      <div class="section">
        <div class="title">
          More Advanced Dynamics for More Advanced Segmentations
        </div>

        <p>One of the issues with just using the Cahn-Hilliard equation for segmentation of superalloys is that it does not account for precipitates being square. Many superalloy microstructures <img src="squares"> have very sharp corners, and this leads to situations where the segmentation is unsatisfactory. To counter this, we simply replace the Cahn-Hilliard PDE with more complicated PDEs. A good paper to look at to understand how to derive more advanced spinodal decomposition dynamics is <a href="http://www.sciencedirect.com/science/article/pii/S1359645498000159">this one</a> [Y. Wang].</p>

        <p>The equations in this paper are somewhat more complex. In the end you get a system of PDEs that are defined by the following equations:</p>
        
        <ul class="eqs">
          <li><div class="eq">\frac{\partial \eta}{\partial t} = \phi_1 \Delta \eta - \frac{\partial f}{\partial \eta}</div></li>
          <li><div class="eq">\frac{\partial c}{\partial t} = \chi \Delta \left ( \phi_2 \Delta c - \frac{\partial f}{\partial c} - \mu V \ast c\right )</div></li>
          <li><div class="eq">f\left(c, \eta \right ) = \frac{1}{2}b_0 \left ( c - c_1 \right )^2 + \frac{1}{2}b_2 \left ( c_2 - c \right ) \eta^2 + \frac{1}{3} b_3 \eta^3 + \frac{1}{4} b_4 \eta^4</div></li>
          <li><div class="eq">\tilde{V} = \Phi(\mathbf{e}) - \left \langle \Phi(\mathbf{e}) \right \rangle</div></li>
          <li><div class="eq">\tilde{V} = FFT(V)</div></li>
          <li><div class="eq">\mathbf{e} = \frac{\mathbf{k}}{\left | \mathbf{k} \right |}</div></li>
          <li><div class="eq">\Phi \left ( \mathbf{e} \right ) = \frac{1 + 2 \left ( \frac{c_{11} +c_{12} - c_{44}}{c_{44}} \right ) e_1^2 e_2^2}{1 + \left (\frac{c_{11} + c_{12}}{c_{11}} \right)\left (\frac{c_{11} +c_{12} - c_{44}}{c_{44}} \right) e_1^2 e_2^2}</div></li>
        </ul>

        <p>A demonstration of segmenting an image with Cahn-Hilliard vs. these equations can be seen in <img src="chvsyw.png">. Look carefully at the precipitates, especially the labeled ones to understand the difference in the output. Hopefully you are convinced that these PDEs segment things differently than the Cahn-Hilliard PDEs did.</p>
        <p>The issue with these equations is parameterizations. The Cahn-Hilliard equation above had one parameter which determines the basic interface width in a segmentation. The extended equations come with about 7-8 extra free parameters which determine the sharpness of the corners and the orientations of the precipitates. Choosing values for this is non-trivial. A demonstration of choosing different values can be seen in <img src="parameterization.png">. Hopefully this is a convincing demonstration of the importance in parameterizing the segmentation properly.</p>
      </div>
    </div>
</div>
</body>
</html>
